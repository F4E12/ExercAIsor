{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T06:19:39.566985Z","iopub.status.busy":"2024-10-21T06:19:39.566640Z","iopub.status.idle":"2024-10-21T06:19:39.576178Z","shell.execute_reply":"2024-10-21T06:19:39.575118Z","shell.execute_reply.started":"2024-10-21T06:19:39.566953Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['Question'], dtype='object')\n"]}],"source":["import pandas as pd\n","\n","# Load the CSV file\n","file_path = '/kaggle/input/math-dataset-raw-3/updated_questions_v2_extended.csv'\n","df = pd.read_csv(file_path)\n","print(df.columns)\n","# Convert the 'Question' column into a list\n","base_instructions = df.loc[212:, 'Question'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-21T00:47:02.970491Z","iopub.status.busy":"2024-10-21T00:47:02.970074Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c56d125ed7049e4b06f3a882d2b582a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Generating Variations:   0%|          | 0/767 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","Generating Variations:   0%|          | 1/767 [00:32<6:56:07, 32.59s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","Generating Variations:  94%|█████████▍| 721/767 [5:11:25<22:43, 29.64s/it]  "]}],"source":["from time import time\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","from tqdm import tqdm\n","import json\n","\n","# ================== Model Configuration ==================\n","model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    return_dict=True,\n","    low_cpu_mem_usage=True,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","llama31_pipeline = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","\n","# ==========================================================\n","\n","# ================== Helper Functions ==================\n","\n","def query_model(system_message, user_message, temperature=0.7, max_length=1024):\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_message},\n","        {\"role\": \"user\", \"content\": \"Create a variation: \" + user_message}\n","    ]\n","    prompt_text = tokenizer.apply_chat_template(\n","        messages, \n","        tokenize=False, \n","        add_generation_prompt=True\n","    )\n","    terminators = [\n","        tokenizer.eos_token_id,\n","        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n","    sequences = llama31_pipeline(\n","        prompt_text,\n","        do_sample=True,\n","        top_p=0.9,\n","        temperature=temperature,\n","        num_return_sequences=1,\n","        eos_token_id=terminators,\n","        max_new_tokens=max_length,\n","        return_full_text=False,\n","        pad_token_id=terminators[0]\n","    )\n","    answer = sequences[0]['generated_text']\n","    return answer\n","\n","system_message = \"\"\"\n","You are an AI trained to generate variations of math problems.\n","Please generate a new version of the problem statement by changing names, numbers, and items, but do not solve the problem and not adding any other uneseccary string, just the variation.\n","And make sure it can be solved and ensure that each variation you generate includes the question.\n","\"\"\"\n","\n","# ==========================================================\n","\n","# ================== Data Generation ==================\n","\n","num_variations_per_instruction = 10\n","output_file = \"math_variations_dataset50_2.jsonl\"\n","\n","data_points = []\n","\n","for base_instruction in tqdm(base_instructions, desc=\"Generating Variations\"):\n","    for _ in range(num_variations_per_instruction):\n","        user_message = base_instruction\n","        variation = query_model(system_message, user_message, temperature=0.7, max_length=256)\n","        data_point = {\n","            \"instruction\": base_instruction,\n","            \"response\": variation,\n","        }\n","        data_points.append(data_point)\n","\n","# Save to JSONL file\n","with open(output_file, 'w') as file:\n","    for data_point in data_points:\n","        json_line = json.dumps(data_point)\n","        file.write(json_line + \"\\n\")\n","\n","print(f\"Generated and saved {len(data_points)} data points to {output_file}.\")\n","\n","# ==========================================================\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T15:42:22.589926Z","iopub.status.busy":"2024-10-20T15:42:22.589482Z","iopub.status.idle":"2024-10-20T15:42:22.616336Z","shell.execute_reply":"2024-10-20T15:42:22.615428Z","shell.execute_reply.started":"2024-10-20T15:42:22.589884Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated and saved 2110 data points to math_variations_dataset50_2.jsonl.\n"]}],"source":["# Save to JSONL file\n","import json\n","with open(output_file, 'w') as file:\n","    for data_point in data_points:\n","        json_line = json.dumps(data_point)\n","        file.write(json_line + \"\\n\")\n","\n","print(f\"Generated and saved {len(data_points)} data points to {output_file}.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5904751,"sourceId":9664172,"sourceType":"datasetVersion"},{"datasetId":5913093,"sourceId":9675222,"sourceType":"datasetVersion"},{"modelId":91102,"modelInstanceId":68809,"sourceId":104449,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
